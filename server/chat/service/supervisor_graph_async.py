# server/chat/service/supervisor_graph_async.py - ë¹„ë™ê¸° Supervisor Graph
from typing import TypedDict, Optional
from dotenv import load_dotenv
from langgraph.graph import StateGraph
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
import asyncio

import server.chat.service.groq_subgraph as groq_subgraph
from server.chat.service.tts_service import generate_tts_audio
from server.chat.service.chat_logic_service_async import handle_chat_flow_async
from server.core.executor import run_in_threadpool

from transformers import pipeline

load_dotenv()

# ============================================================================
# âœ… ëª¨ë¸ ì „ì—­ ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
# ============================================================================
print("ğŸ”„ Loading CEFR classifier model...")
cefr_classifier = pipeline(
    "text-classification",
    model="dksysd/cefr-classifier",
    tokenizer="dksysd/cefr-classifier"
)
print("âœ… CEFR classifier loaded")

# LLM ëª¨ë¸ (ë¹„ë™ê¸° ì‚¬ìš© ê°€ëŠ¥)
CHAT_GENERATE_LLM = ChatOpenAI(model="gpt-4o")
SUMMARY_LLM = ChatOpenAI(model="gpt-4o-mini")
ANALYSIS_LLM = ChatOpenAI(model="gpt-4o-mini")

supervisor_llm = ChatOpenAI(model="gpt-4o")
podcast_app = groq_subgraph.build_podcast_graph()


class SupervisorState(TypedDict, total=False):
    user_input: str
    output: str
    audio_base64: str
    route: str
    userId: int
    chatNum: int
    chatOrder: int
    initialChat: bool
    history: str
    history_summary: str
    cefr_level: str


# ============================================================================
# âœ… ë¼ìš°íŒ… ê²°ì • (ë¹„ë™ê¸°)
# ============================================================================
async def route_decision(state: SupervisorState) -> SupervisorState:
    """podcast or chat ë¶„ê¸° ê²°ì • (ë¹„ë™ê¸°)"""
    msg = [
        SystemMessage("""
            You are a routing assistant.
            Your task is to decide whether the user's input should be handled by the podcast generator or by the general chat assistant.

            1. When a user requests to create a podcast
            2. When the user wants to practice listening
            3. When a user requests a listening activity, such as a radio show

            In the above three cases, branch out to "podcast"

            Otherwise â€” for general questions, discussions, explanations, or normal chat â€”
            route to "chat".

            Respond with only one word: either "podcast" or "chat".
        """),
        HumanMessage(state["user_input"])
    ]

    # âœ… ë¹„ë™ê¸° LLM í˜¸ì¶œ
    response = await supervisor_llm.ainvoke(msg)
    route_raw = response.content.strip().lower()
    route = "podcast" if "podcast" in route_raw else "chat"

    print(f"[ROUTE] ğŸ”€ Decided: {route}")
    return {**state, "route": route}


# ============================================================================
# âœ… íŒŸìºìŠ¤íŠ¸ ì‹¤í–‰ (ë™ê¸° ì½”ë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘)
# ============================================================================
async def run_podcast(state: SupervisorState) -> SupervisorState:
    """íŒŸìºìŠ¤íŠ¸ ë¶„ê¸° (ë¹„ë™ê¸°)"""
    # TODO: podcast_appë„ ë¹„ë™ê¸°ë¡œ ì „í™˜ í•„ìš”
    # í˜„ì¬ëŠ” thread poolì—ì„œ ì‹¤í–‰
    res = await run_in_threadpool(
        podcast_app.invoke,
        {
            "user_input": state["user_input"],
            "history": "",
            "history_summary": "Radio show is started. You need to speak",
            "turn_count": 0
        }
    )
    script = res.get("history", "")

    # TTSëŠ” ë™ê¸°ì´ë¯€ë¡œ thread poolì—ì„œ ì‹¤í–‰
    audio = await run_in_threadpool(generate_tts_audio, script)

    return {**state, "output": script, "audio_base64": audio, "route": "podcast"}


# ============================================================================
# âœ… CEFR ë ˆë²¨ ì˜ˆì¸¡ (CPU ì§‘ì•½ì , thread poolì—ì„œ ì‹¤í–‰)
# ============================================================================
async def predict_cefr_level_async(user_input: str) -> str:
    """
    HuggingFace CEFR ë¶„ë¥˜ ëª¨ë¸ë¡œ ë¬¸ì¥ì˜ CEFR ë ˆë²¨ì„ ì˜ˆì¸¡ (ë¹„ë™ê¸°)

    âœ… CPU ì§‘ì•½ì  ì‘ì—…ì´ë¯€ë¡œ thread poolì—ì„œ ì‹¤í–‰
    """
    try:
        # âœ… transformers.pipelineì€ CPU ì§‘ì•½ì ì´ë¯€ë¡œ thread poolì—ì„œ ì‹¤í–‰
        result = await run_in_threadpool(cefr_classifier, user_input)
        label = result[0]["label"]
        print(f"[CEFR] ğŸ“Š Predicted: {label}")
        return label  # "A1" ~ "C2"
    except Exception as e:
        print(f"[CEFR] âŒ Error: {e}")
        return "UNKNOWN"


# ============================================================================
# âœ… ì±„íŒ… ì‹¤í–‰ (ì™„ì „ ë¹„ë™ê¸°)
# ============================================================================
async def run_chat(state: SupervisorState) -> SupervisorState:
    """
    ì±„íŒ… í”Œë¡œìš° (ì™„ì „ ë¹„ë™ê¸°)

    âœ… ìµœì í™” í¬ì¸íŠ¸:
    - CEFR ë¶„ë¥˜: Thread poolì—ì„œ ì‹¤í–‰
    - DB ì¿¼ë¦¬: AsyncSession ì‚¬ìš©
    - LLM í˜¸ì¶œ: ainvoke() ì‚¬ìš©
    """
    user_input = state.get("user_input", "")

    # âœ… 1ë‹¨ê³„: CEFR ë ˆë²¨ ì˜ˆì¸¡ (ë¹„ë™ê¸°)
    if user_input:
        cefr_level = await predict_cefr_level_async(user_input)
        state["cefr_level"] = cefr_level

    # âœ… 2ë‹¨ê³„: ì±„íŒ… í”Œë¡œìš° ì‹¤í–‰ (ë¹„ë™ê¸°)
    result = await handle_chat_flow_async(
        state=state,
        chat_llm=CHAT_GENERATE_LLM,
        summary_llm=SUMMARY_LLM,
        analysis_llm=ANALYSIS_LLM
    )

    return {
        **state,
        **result,
        "route": "chat",
        "cefr_level": state.get("cefr_level")
    }


# ============================================================================
# âœ… Supervisor Graph ë¹Œë“œ
# ============================================================================
def build_supervisor_graph():
    """
    ë¹„ë™ê¸° Supervisor Graph ë¹Œë“œ

    âš ï¸ LangGraphëŠ” ë™ê¸°/ë¹„ë™ê¸° ëª¨ë‘ ì§€ì›í•˜ì§€ë§Œ,
       ê° ë…¸ë“œ í•¨ìˆ˜ë¥¼ async defë¡œ ì •ì˜í•´ì•¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ë¨
    """
    g = StateGraph(SupervisorState)
    g.add_node("route_decision", route_decision)
    g.add_node("podcast", run_podcast)
    g.add_node("chat", run_chat)

    g.set_entry_point("route_decision")
    g.add_conditional_edges("route_decision", lambda s: s["route"], {
        "podcast": "podcast",
        "chat": "chat"
    })
    return g.compile()
