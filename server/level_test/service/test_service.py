# # server/test/service/test_service.py
# from langchain_openai import ChatOpenAI
# from server.level_test.repository.test_repository import save_level_test_log

# # âœ… ì„ì‹œ ì €ì¥ì†Œ (ì‹¤ì œ ì„œë¹„ìŠ¤ë©´ Redisë‚˜ DBë¡œ êµì²´ ê°€ëŠ¥)
# test_state = {
#     "cnt": 0,
#     "history": [],
#     "history_summary" : []
# }

# # âœ… í…ŒìŠ¤íŠ¸ìš© LLM (Ollama Qwen)
# test_llm = ChatOpenAI(
#     model="qwen:4b",
#     base_url="http://127.0.0.1:11434/v1",
#     api_key="none"
# )

# # âœ… ê²°ê³¼ ë¶„ì„ìš© LLM (GPT-4o)
# result_llm = ChatOpenAI(model="gpt-4o")
# summary_llm = ChatOpenAI(model="gpt-4o-mini")

# # ===============================
# # 1ï¸âƒ£ ì–´íœ˜ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ (/test)
# # ===============================
# async def process_test_message(db, login_id: str, message: str):
#     # 1ï¸âƒ£ ì¹´ìš´íŠ¸ ì¦ê°€
#     #test_state["cnt"] += 1

#     # 2ï¸âƒ£ ëŒ€í™” context êµ¬ì„±: summary + ìµœê·¼ ë©”ì‹œì§€
#     context = f"""
#     Summary of previous conversation:
#     {test_state['history_summary']}

#     Last messages:
#     {test_state['history'][-1]['user'] if test_state['history'] else ''}
#     {test_state['history'][-1]['ai'] if test_state['history'] else ''}

#     User now says: {message}
#     Respond naturally, in 1-2 sentences, friendly and conversational.
#     When you answer, you don't need to provide information, but simply answer briefly just for socializing.
#     Answer by empathizing or asking about the condition of the user
#     """

#     # 3ï¸âƒ£ AI ì‘ë‹µ ìƒì„±
#     response = test_llm.invoke(context)

#     # 4ï¸âƒ£ ëŒ€í™” ê¸°ë¡ ì €ì¥
#     test_state["history"].append({
#         "user": message,
#         "ai": response.content
#     })

#     save_level_test_log(
#         db=db,
#         login_id=login_id,
#         user_question=message,
#         ai_response=response.content,
#     )

#     # 5ï¸âƒ£ Summary ì—…ë°ì´íŠ¸ (ìš”ì•½ LLMì—ê²Œ ì „ë‹¬)
#     summary_prompt = f"""
#     Update this conversation summary based on the new exchange.

#     Old summary:
#     {test_state['history_summary']}

#     New message:
#     User: {message}
#     AI: {response.content}

#     Provide an updated concise summary that keeps the important topics and tone.
#     """
#     summary_response = summary_llm.invoke(summary_prompt)
#     test_state["history_summary"] = summary_response.content.strip()

#     # 6ï¸âƒ£ ê²°ê³¼ ë°˜í™˜
#     return {
#         "cnt": test_state["cnt"],
#         "user_message": message,
#         "llm_reply": response.content,
#         "summary": test_state["history_summary"]
#     }


# # ===============================
# # 2ï¸âƒ£ ê²°ê³¼ ë¶„ì„ (/test-result)
# # ===============================
# async def analyze_test_result():
#     # if test_state["cnt"] < 100:
#     #     return {"error": "í…ŒìŠ¤íŠ¸ê°€ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ì–´ìš”.", "current_cnt": test_state["cnt"]}

#     # ì–´íœ˜ë ¥ í‰ê°€ í”„ë¡¬í”„íŠ¸
#     prompt = f"""
#     ë‹¤ìŒì€ userê°€ ì–´íœ˜ë ¥ í…ŒìŠ¤íŠ¸ ì¤‘ ë‚¨ê¸´ 100ê°œì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.
#     ì´ë¥¼ ì¢…í•©í•˜ì—¬ userëŠ” CEFR ê¸°ì¤€(A1~C2) ì¤‘ ì–´ëŠ ìˆ˜ì¤€ì˜ ì–´íœ˜ë ¥ì„ ë³´ì´ëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.
#     history:
#     {test_state["history"]}
#     """

#     # âœ… ë¶„ì„ ì™„ë£Œ í›„ ìƒíƒœ ì´ˆê¸°í™”
#     test_state["cnt"] = 0
#     test_state["history"] = []

#     result = result_llm.invoke(prompt)


#     return {
#         "level_analysis": result.content,
#         "total_messages": len(test_state["history"])
#     }


from langchain_openai import ChatOpenAI
from server.level_test.repository.log_repository import (
    get_user_by_login_id, get_last_log,
    get_recent_logs, get_all_logs_by_level,
    save_level_test_log
)
from server.level_test.repository.summary_repository import (
    get_summaries_by_level, get_last_summary, save_summary
)
from datetime import datetime
import httpx
import os

test_llm = ChatOpenAI(model="qwen:4b", base_url="http://127.0.0.1:11434/v1", api_key="none")
summary_llm = ChatOpenAI(model="gpt-4o-mini")
result_llm = ChatOpenAI(model="gpt-4o")

# Spring Boot API URL
SPRING_BOOT_URL = os.getenv("SPRING_BOOT_URL", "http://localhost:8080")


async def evaluate_level(db, user_id: int, level_test_num: int) -> str:
    """ìµœê·¼ 10ê°œ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ CEFR ë ˆë²¨ í‰ê°€"""
    last_ten = get_recent_logs(db, user_id, level_test_num, 10)

    if not last_ten:
        print("âš ï¸ í‰ê°€í•  ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. Beginner ë°˜í™˜")
        return "Beginner"

    if len(last_ten) < 10:
        print(f"âš ï¸ ëŒ€í™”ê°€ 10ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤ (í˜„ì¬: {len(last_ten)}ê°œ)")

    dialogue_text = "\n".join([f"User: {x.user_question}\nAI: {x.ai_response}" for x in last_ten])

    prompt = f"""Analyze the following {len(last_ten)} exchanges and determine the user's English proficiency level (CEFR: A1, A2, B1, B2, C1, C2).
Consider vocabulary richness, grammar complexity, sentence structure, and fluency.

Evaluation criteria:
- Beginner: Very basic English, simple words, many errors
- A1: Basic phrases, simple vocabulary
- A2: Elementary level, can describe familiar matters
- B1: Intermediate level, can handle most travel situations
- B2: Upper-intermediate, can interact with fluency
- C1: Advanced, can express ideas fluently
- C2: Proficient, near-native level

Dialogue:
{dialogue_text}

Respond with ONLY ONE of these exact words: Beginner, A1, A2, B1, B2, C1, or C2.
No other text, just the level."""

    try:
        print("ğŸ¤– GPT-4o-miniì—ê²Œ ë ˆë²¨ í‰ê°€ ìš”ì²­ ì¤‘...")
        response = summary_llm.invoke(prompt)  # gpt-4o-mini
        level = response.content.strip()
        print(f"ğŸ¤– GPT ì›ë³¸ ì‘ë‹µ: '{level}'")

        # ìœ íš¨í•œ ë ˆë²¨ì¸ì§€ í™•ì¸
        valid_levels = ["Beginner", "A1", "A2", "B1", "B2", "C1", "C2"]
        if level not in valid_levels:
            print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë ˆë²¨ ì‘ë‹µ, í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„...")
            # ë ˆë²¨ì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
            for valid_level in valid_levels:
                if valid_level in level:
                    level = valid_level
                    print(f"âœ… ì¶”ì¶œ ì„±ê³µ: {level}")
                    break
            else:
                print(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨, Beginnerë¡œ ì„¤ì •")
                level = "Beginner"  # ê¸°ë³¸ê°’

        return level
    except Exception as e:
        print(f"âŒ ë ˆë²¨ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "Beginner"


async def update_user_rank_in_spring(user_id: int, rank_title: str) -> bool:
    """Spring Boot APIë¥¼ í˜¸ì¶œí•˜ì—¬ Userì˜ rank ì—…ë°ì´íŠ¸"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.patch(
                f"{SPRING_BOOT_URL}/api/v1/users/{user_id}/rank",
                json={"rankTitle": rank_title},
                timeout=10.0
            )
            if response.status_code == 200:
                print(f"âœ… User {user_id} rank updated to {rank_title}")
                return True
            else:
                print(f"âŒ Failed to update rank: {response.status_code} - {response.text}")
                return False
    except Exception as e:
        print(f"âš ï¸ Error updating rank in Spring Boot: {e}")
        return False


async def process_test_message(db, login_id: str, message: str):

    # ëª‡ë²ˆì§¸ ëŒ€í™”ì¸ì§€ í™•ì¸í•˜ê¸°
    # 1ï¸âƒ£ ì‚¬ìš©ì ë° ìµœê·¼ ë¡œê·¸
    user = get_user_by_login_id(db, login_id)
    if not user:
        raise ValueError("User not found")

    user_id = user.id
    last_log = get_last_log(db, user_id)

    # 2ï¸âƒ£ level_test_num, dialog_num ê³„ì‚°
    if not last_log:
        level_test_num, dialog_num = 1, 1
    else:
        if last_log.diolog_num >= 100:
            level_test_num, dialog_num = last_log.level_test_num + 1, 1
        else:
            level_test_num, dialog_num = last_log.level_test_num, last_log.diolog_num + 1




    
    # 10ê°œ ë‹¨ìœ„ ìš”ì•½ + ë‚±ê°œ ìš”ì•½ ì•ˆëœ ëŒ€í™” ë¶ˆëŸ¬ì™€ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    # 3ï¸âƒ£ ìš”ì•½ ë¶ˆëŸ¬ì˜¤ê¸° (ëª¨ë“  summary)
    summaries = get_summaries_by_level(db, user_id, level_test_num)
    summary_context = "\n".join([s.summary_text for s in summaries])

    # 4ï¸âƒ£ ìµœê·¼ n%10 ëŒ€í™” ê¸°ë¡
    remainder = (dialog_num - 1) % 10
    recent_logs = get_recent_logs(db, user_id, level_test_num, remainder)
    dialogue_context = "\n".join(
        [f"User: {l.user_question}\nAI: {l.ai_response}" for l in recent_logs]
    )

    # 5ï¸âƒ£ context ìƒì„±
    context = f"""
    Summary of previous conversation:
    {summary_context}

    Recent exchanges:
    {dialogue_context}

    User now says: {message}
    Respond naturally, in 1-2 sentences, friendly and conversational.
    When you answer, you don't need to provide information, but simply answer briefly just for socializing.
    Answer by empathizing or asking about the condition of the user
    """





    # invoke í•˜ê¸°
    # 6ï¸âƒ£ AI ì‘ë‹µ ìƒì„±
    response = test_llm.invoke(context)
    ai_reply = response.content.strip()





    # 7ï¸âƒ£ ë¡œê·¸ ì €ì¥
    new_log = save_level_test_log(db, user_id, message, ai_reply, level_test_num, dialog_num)

    # ğŸ†• í˜„ì¬ ë ˆë²¨ ì •ë³´ (ê¸°ë³¸ê°’)
    current_level = user.ranks.title if user.ranks else "Beginner"
    level_changed = False

    # 8ï¸âƒ£ ìš”ì•½ ì €ì¥ (10ì˜ ë°°ìˆ˜ì¼ë•Œë§Œ)
    if dialog_num % 10 == 0:
        print(f"\n{'='*60}")
        print(f"ğŸ”Ÿ 10ë²ˆì§¸ ëŒ€í™” ë„ë‹¬! (ëŒ€í™” ë²ˆí˜¸: {dialog_num})")
        print(f"{'='*60}\n")

        # 10ê°œ ëŒ€í™” ìš”ì•½
        last_ten = get_recent_logs(db, user_id, level_test_num, 10)
        text = "\n".join([f"User: {x.user_question}\nAI: {x.ai_response}" for x in last_ten])
        prompt = f"Summarize the following 10 exchanges concisely:\n{text}"
        summary_text = summary_llm.invoke(prompt).content.strip()

        last_summary = get_last_summary(db, user_id, level_test_num)
        next_summary_num = (last_summary.summary_num + 1) if last_summary else 1
        save_summary(db, user_id, level_test_num, next_summary_num, summary_text)
        print(f"âœ… ìš”ì•½ ì €ì¥ ì™„ë£Œ (ìš”ì•½ ë²ˆí˜¸: {next_summary_num})")

        # ğŸ†• ë ˆë²¨ í‰ê°€ ìˆ˜í–‰
        previous_level = user.ranks.title if user.ranks else "Beginner"
        print(f"ğŸ“Š ë ˆë²¨ í‰ê°€ ì‹œì‘...")
        print(f"   - í˜„ì¬ ë ˆë²¨: {previous_level}")
        print(f"   - í‰ê°€ ëŒ€ìƒ: ìµœê·¼ 10ê°œ ëŒ€í™”")

        evaluated_level = await evaluate_level(db, user_id, level_test_num)
        print(f"   - í‰ê°€ ê²°ê³¼: {evaluated_level}")

        # ë ˆë²¨ì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ Spring Bootë¡œ ì—…ë°ì´íŠ¸
        if evaluated_level != previous_level:
            print(f"ğŸ”„ ë ˆë²¨ ë³€ê²½ ê°ì§€! {previous_level} â†’ {evaluated_level}")
            success = await update_user_rank_in_spring(user.id, evaluated_level)
            if success:
                current_level = evaluated_level
                level_changed = True
                # DBì—ì„œ userì˜ rank ì •ë³´ ì—…ë°ì´íŠ¸ (ìºì‹œ ë™ê¸°í™”)
                db.refresh(user)
                print(f"ğŸ‰ ë ˆë²¨ ì—…ë°ì´íŠ¸ ì„±ê³µ! ìƒˆ ë ˆë²¨: {evaluated_level}")
            else:
                print(f"âŒ Spring Boot ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                current_level = previous_level
        else:
            print(f"âœ… ë ˆë²¨ ìœ ì§€: {previous_level}")
            current_level = previous_level

        print(f"\n{'='*60}\n")

    # 9ï¸âƒ£ 100íšŒ ë„ë‹¬ ì‹œ ê²°ê³¼ ë¶„ì„
    if dialog_num == 100:
        await analyze_test_result(db, login_id, level_test_num)

    return {
        "user_message": message,
        "llm_reply": ai_reply,
        "level_test_num": level_test_num,
        "dialog_num": dialog_num,
        "current_level": current_level,
        "level_changed": level_changed
    }


async def analyze_test_result(db, login_id: str, level_test_num: int):
    user = get_user_by_login_id(db, login_id)
    logs = get_all_logs_by_level(db, user.id, level_test_num)

    history_text = "\n".join(
        [f"User: {x.user_question}\nAI: {x.ai_response}" for x in logs]
    )

    prompt = f"""
    ë‹¤ìŒì€ userê°€ ì–´íœ˜ë ¥ í…ŒìŠ¤íŠ¸ ì¤‘ ë‚¨ê¸´ 100ê°œì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.
    ì´ë¥¼ ì¢…í•©í•˜ì—¬ userëŠ” CEFR ê¸°ì¤€(A1~C2) ì¤‘ ì–´ëŠ ìˆ˜ì¤€ì˜ ì–´íœ˜ë ¥ì„ ë³´ì´ëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.
    history:
    {history_text}
    """

    result = result_llm.invoke(prompt)
    return {"level_analysis": result.content.strip()}
